import os
import sys
import platform
import pickle
import time
import logging
import tempfile
from pathlib import Path

import requests
import pyperclip

from PIL import Image
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

from apscheduler.triggers.cron import CronTrigger
from apscheduler.schedulers.blocking import BlockingScheduler

# -----------------------
# Config
# -----------------------
THREADS_COOKIE_FILE = "threads_cookies.pkl"
NEWSPIC_COOKIE_FILE = "newspic_cookies.pkl"

# í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥ (ê¸°ë³¸ True: ì„œë²„ëŠ” ë³´í†µ í—¤ë“œë¦¬ìŠ¤)
USE_HEADLESS = os.getenv("USE_HEADLESS", "true").lower() == "true"

# íƒ€ì„ì¡´
SCHED_TZ = "Asia/Seoul"

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    stream=sys.stdout,
)

BASE_DIR = Path(__file__).resolve().parent


def is_linux():
    return platform.system().lower() == "linux"


def build_chrome():
    options = webdriver.ChromeOptions()
    # ì„œë²„/ì»¨í…Œì´ë„ˆ ì•ˆì • í”Œë˜ê·¸
    if USE_HEADLESS:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1366,768")
    # í•„ìš” ì‹œ UA ì§€ì •
    # options.add_argument("user-agent=Mozilla/5.0 ...")

    # ë‹¤ìš´ë¡œë“œ ìºì‹œê°€ ëŠë¦¬ë©´ WDM ìºì‹œ ê²½ë¡œ ì§€ì •ë„ ê°€ëŠ¥
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options,
    )
    return driver


class Browser:
    def __init__(self, cookie_file, url):
        self.cookie_file = str(BASE_DIR / cookie_file)
        self.url = url
        self.driver = None
        self.get_driver()

    def get_driver(self):
        if not self.driver:
            self.driver = build_chrome()
        return self.driver

    def _wait(self, condition, timeout=15):
        return WebDriverWait(self.driver, timeout).until(condition)

    def open_browser(self):
        """
        - ì¿ í‚¤ê°€ ìˆìœ¼ë©´: url ì˜¤í”ˆ â†’ ì¿ í‚¤ ë¡œë“œ â†’ ìƒˆë¡œê³ ì¹¨
        - ì—†ìœ¼ë©´:
            * í—¤ë“œë¦¬ìŠ¤ì¸ ê²½ìš°: ë¡œê·¸ì¸ ë¶ˆê°€í•˜ë‹ˆ ì˜ˆì™¸ ì•ˆë‚´
            * í—¤ë“œë¦¬ìŠ¤ê°€ ì•„ë‹ˆë©´: ìˆ˜ë™ ë¡œê·¸ì¸ ìœ ë„ í›„ ì¿ í‚¤ ì €ì¥
        """
        self.driver.get(self.url)
        time.sleep(2)

        if not os.path.exists(self.cookie_file):
            if USE_HEADLESS:
                raise RuntimeError(
                    f"[cookie] '{self.cookie_file}' ê°€ ì—†ê³  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì…ë‹ˆë‹¤. "
                    "í—¤ë“œë¦¬ìŠ¤ì—ì„œëŠ” ìˆ˜ë™ ë¡œê·¸ì¸ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                    "ë¡œì»¬(ë¹„í—¤ë“œë¦¬ìŠ¤)ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸ í›„ ì¿ í‚¤ íŒŒì¼ì„ ì„œë²„ì— ì—…ë¡œë“œí•˜ì„¸ìš”."
                )
            else:
                self.login_and_save_session()
        else:
            try:
                self.load_cookies()
                self.driver.refresh()
                time.sleep(3)
                logging.info("âœ… ì¿ í‚¤ ë¡œë“œ í›„ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")
            except Exception as e:
                logging.warning(f"[cookie] ë¡œë“œ ì‹¤íŒ¨ â†’ ì¬ë¡œê·¸ì¸ ì‹œë„: {e}")
                if USE_HEADLESS:
                    raise RuntimeError(
                        "í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œ ì¿ í‚¤ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
                        "ë¡œì»¬ì—ì„œ ì¿ í‚¤ë¥¼ ê°±ì‹ í•´ ì„œë²„ì— ë‹¤ì‹œ ë°°í¬í•˜ì„¸ìš”."
                    )
                self.login_and_save_session()

    def login_and_save_session(self):
        self.driver.get(self.url)
        print("ğŸ” ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸í•œ ë’¤ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        input("â–¶ ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enter: ")
        self.save_cookies()

    def save_cookies(self):
        with open(self.cookie_file, "wb") as file:
            pickle.dump(self.driver.get_cookies(), file)
        logging.info("âœ… ì¿ í‚¤ ì €ì¥ ì™„ë£Œ: %s", self.cookie_file)

    def load_cookies(self):
        with open(self.cookie_file, "rb") as file:
            cookies = pickle.load(file)
            for cookie in cookies:
                # selenium 4ì—ì„œ 'sameSite' ë¬¸ì œ íšŒí”¼
                if "sameSite" in cookie:
                    del cookie["sameSite"]
                # ìœ íš¨ ë„ë©”ì¸ì—ì„œë§Œ add_cookie ê°€ëŠ¥ -> ì´ë¯¸ self.url ë¡œ ì ‘ì†ëœ ìƒíƒœ
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    logging.debug(f"[cookie] add_cookie ì‹¤íŒ¨: {e}")
        logging.info("âœ… ì¿ í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")

    def switch_tab(self, url_keyword, timeout=10):
        """
        ê¸°ì¡´ ë¡œì§ ìœ ì§€(ì ˆëŒ€ XPATH ê·¸ëŒ€ë¡œ ì“°ê¸° ìœ„í•¨).
        íƒ­ ì „í™˜ ì•ˆì •í™”: ì¼ì • ì‹œê°„ ì•ˆì— keywordë¡œ ì‹œì‘í•˜ëŠ” URL íƒ­ ì „í™˜.
        """
        end = time.time() + timeout
        while time.time() < end:
            for handle in self.driver.window_handles:
                self.driver.switch_to.window(handle)
                time.sleep(0.5)
                try:
                    cur = self.driver.current_url
                except Exception:
                    continue
                if cur.startswith(url_keyword):
                    time.sleep(0.8)
                    return True
            time.sleep(0.4)
        raise TimeoutException(f"[tab] '{url_keyword}' ë¡œ ì‹œì‘í•˜ëŠ” íƒ­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    def find(self, by, element, sleep=0.0, timeout=15, clickable=False):
        """
        ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë˜ ë‚´ë¶€ëŠ” WebDriverWait ê¸°ë°˜.
        - ì ˆëŒ€ XPATH ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
        - clickable=Trueë©´ í´ë¦­ ê°€ëŠ¥ ìƒíƒœê¹Œì§€ ëŒ€ê¸°
        """
        try:
            if clickable:
                el = self._wait(EC.element_to_be_clickable((by, element)), timeout=timeout)
            else:
                el = self._wait(EC.presence_of_element_located((by, element)), timeout=timeout)
            if sleep > 0:
                time.sleep(sleep)
            return el
        except TimeoutException:
            logging.warning(f"[find] timeout: {by} {element}")
            return None
        except Exception as e:
            logging.warning(f"[find] error: {e}")
            return None

    def close_other_tabs(self):
        """
        í˜„ì¬ íƒ­ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë¨¸ì§€ íƒ­ ì •ë¦¬ (ì›ë˜ ë¡œì§ ìœ ì§€)
        """
        current_url = self.driver.current_url
        current_handle = ""

        for handle in list(self.driver.window_handles):
            self.driver.switch_to.window(handle)
            time.sleep(0.2)
            try:
                if self.driver.current_url != current_url:
                    self.driver.close()
                    time.sleep(0.2)
                else:
                    current_handle = handle
            except Exception:
                # ì´ë¯¸ ë‹«í˜”ê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€
                pass

        if current_handle:
            self.driver.switch_to.window(current_handle)

    def terminate(self):
        if self.driver:
            try:
                self.driver.quit()
            except Exception:
                pass
        self.driver = None


def safe_get_article_url(driver):
    """
    pyperclip(í´ë¦½ë³´ë“œ)ì´ ì—†ëŠ” ì„œë²„ ëŒ€ë¹„:
    canonical / og:url ë©”íƒ€ ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ í˜„ì¬ URL
    """
    try:
        el = driver.find_element(By.XPATH, "//link[@rel='canonical']")
        href = el.get_attribute("href")
        if href:
            return href
    except Exception:
        pass
    try:
        el = driver.find_element(By.XPATH, "//meta[@property='og:url']")
        href = el.get_attribute("content")
        if href:
            return href
    except Exception:
        pass
    try:
        return driver.current_url
    except Exception:
        return ""


def download_and_crop(image_url, out_path, crop_height=360, timeout=10):
    """
    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + ìƒë‹¨ crop, ì„ì‹œíŒŒì¼ ì •ë¦¬
    """
    if not image_url:
        return False
    try:
        r = requests.get(image_url, timeout=timeout)
        r.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tf:
            tf.write(r.content)
            tmp = tf.name
        with Image.open(tmp) as img:
            w, h = img.size
            box = (0, 0, w, min(h, crop_height))
            img.crop(box).save(out_path, format="JPEG")
        os.remove(tmp)
        return True
    except Exception as e:
        logging.warning(f"[image] {e}")
        return False


def crawl_newspic_ai_contents():
    start_time = datetime.now().strftime("%m%d%H%M")
    newspic_browser = Browser(NEWSPIC_COOKIE_FILE, "https://partners.newspic.kr/")

    newspic_browser.open_browser()

    # ì „ì²´ í˜ì´ì§€ ìˆ˜
    total_pages_el = newspic_browser.find(By.XPATH, "/html/body/div[6]/main/div[1]/section[2]/div[2]/div/div[1]/div/p/em[3]")
    if not total_pages_el:
        raise RuntimeError("ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    total_pages = int(total_pages_el.text.strip())

    news_info = {}

    for page in range(total_pages):
        for news_index in range(3):
            try:
                el = newspic_browser.find(
                    By.XPATH,
                    f"/html/body/div[6]/main/div[1]/section[2]/div[2]/div/div[2]/ul/li[{news_index + 1}]/div[2]/a",
                    clickable=True,
                )
                if el:
                    el.click()
                else:
                    logging.info("ê¸°ì‚¬ ë§í¬ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
            except Exception as e:
                logging.info(f"ê¸°ì‚¬ í´ë¦­ ì˜ˆì™¸: {e}")
                continue

            unique_id = f"{start_time}_{page + 1}_{news_index + 1}"

            # ê¸°ì‚¬ íƒ­ìœ¼ë¡œ ì „í™˜
            newspic_browser.switch_tab("https://m.newspic.kr/view.html")

            # ë‰´ìŠ¤ íƒ€ì´í‹€
            title_el = newspic_browser.find(By.XPATH, "/html/body/div[2]/div[1]/div[1]/div[2]/h3")
            if not title_el:
                logging.info("ì œëª©ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                newspic_browser.close_other_tabs()
                continue
            news_title = title_el.text.strip()

            # ë§í¬ ë³µì‚¬ ë²„íŠ¼ í´ë¦­ í›„ pyperclip, ì‹¤íŒ¨ ì‹œ fallback
            news_url = ""
            try:
                share_btn = newspic_browser.find(By.XPATH, "/html/body/div[3]/div[2]/ul/li[1]/button", clickable=True)
                if share_btn:
                    share_btn.click()
                    time.sleep(0.8)
                    try:
                        news_url = pyperclip.paste().strip()
                    except Exception:
                        news_url = ""
                if not news_url:
                    news_url = safe_get_article_url(newspic_browser.driver)
            except Exception:
                news_url = safe_get_article_url(newspic_browser.driver)

            # ì´ë¯¸ì§€ë“¤
            img_elements = newspic_browser.driver.find_elements(
                By.XPATH, "/html/body/div[2]/div[1]/div[1]/div[5]//img"
            )
            images = []
            for e in img_elements:
                try:
                    src = e.get_attribute("src") or ""
                    if src.startswith("https://images-cdn.newspic.kr"):
                        images.append(src)
                except Exception:
                    pass

            image_path = str(BASE_DIR / f"{unique_id}.jpg")
            if images:
                ok = download_and_crop(images[0], image_path)
                if not ok:
                    logging.info("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ â†’ ì´ë¯¸ì§€ ì—†ì´ ì§„í–‰")
                    image_path = ""
            else:
                logging.info("ì´ë¯¸ì§€ ì—†ìŒ â†’ ì´ë¯¸ì§€ ì—†ì´ ì§„í–‰")
                image_path = ""

            # íŒŒíŠ¸ë„ˆìŠ¤ë¡œ ë³µê·€ ë° íƒ­ ì •ë¦¬
            newspic_browser.switch_tab("https://partners.newspic.kr")
            newspic_browser.close_other_tabs()

            news_info[unique_id] = (news_title, news_url, image_path)

        # next ë²„íŠ¼
        next_btn = newspic_browser.find(By.XPATH, "/html/body/div[6]/main/div[1]/section[2]/div[2]/div/p/button[2]", clickable=True)
        if next_btn:
            next_btn.click()
            time.sleep(1)
        else:
            logging.info("ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ ë£¨í”„ ì¢…ë£Œ.")
            break

    newspic_browser.terminate()
    return news_info


def upload_news_into_threads(news_info):
    try:
        threads_browser = Browser(THREADS_COOKIE_FILE, "https://threads.net/")
        threads_browser.open_browser()
        time.sleep(3)  # ì´ˆê¸° ë Œë” ì•ˆì •í™”

        for key, (title, url, image) in news_info.items():
            # ê²Œì‹œë²„íŠ¼ í´ë¦­
            e = threads_browser.find(
                By.XPATH,
                "/html/body/div[2]/div/div/div[2]/div[2]/div/div/div/div[1]/div[1]/div[1]/div/div/div[2]/div[1]/div[2]/div/div[2]/div",
                clickable=True,
            )
            if not e:
                logging.info("ê²Œì‹œ ì‹œì‘ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            e.click()
            time.sleep(0.5)

            # ì œëª© ì…ë ¥
            e = threads_browser.find(
                By.XPATH,
                "/html/body/div[2]/div/div/div[3]/div/div/div[1]/div/div[2]/div/div/div/div[2]/div/div/div/div/div/div[3]/div/div/div[1]/div[2]/div[2]/div[1]/p",
            )
            if not e:
                logging.info("ë³¸ë¬¸ ì…ë ¥ì°½ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            e.send_keys(title)
            time.sleep(0.3)

            # ëŒ“ê¸€ í´ë¦­
            e = threads_browser.find(
                By.XPATH,
                "/html/body/div[2]/div/div/div[3]/div/div/div[1]/div/div[2]/div/div/div/div[2]/div/div/div/div/div/div[3]/div/div/div[2]/div[2]/span",
                clickable=True,
            )
            if e:
                e.click()
                time.sleep(0.3)

            # ëŒ“ê¸€ì— ì£¼ì†Œ ì…ë ¥
            e = threads_browser.find(
                By.XPATH,
                "/html/body/div[2]/div/div/div[3]/div/div/div[1]/div/div[2]/div/div/div/div[2]/div/div/div/div/div/div[3]/div[2]/div/div[1]/div[2]/div[2]/div[1]/p",
            )
            if e and url:
                e.send_keys(f"ë‚´ìš© ì´ì–´ì„œ ë³´ê¸°: {url}")
                time.sleep(0.3)

            # ì´ë¯¸ì§€ ì¶”ê°€(ìˆì„ ë•Œë§Œ)
            if image:
                file_input = threads_browser.find(By.XPATH, '//input[@type="file"]')
                if file_input:
                    image_path = str(Path(image).resolve())
                    file_input.send_keys(image_path)
                    time.sleep(1.0)

            # ê²Œì‹œ ë²„íŠ¼ í´ë¦­
            e = threads_browser.find(
                By.XPATH,
                "/html/body/div[2]/div/div/div[3]/div/div/div[1]/div/div[2]/div/div/div/div[2]/div/div/div/div/div/div[4]/div/div[1]/div",
                clickable=True,
            )
            if e:
                e.click()
                time.sleep(3.0)  # ì—…ë¡œë“œ ëŒ€ê¸°
            else:
                logging.info("ìµœì¢… ê²Œì‹œ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    except Exception as e:
        logging.error(f"[threads] ì—…ë¡œë“œ ì¤‘ ì˜ˆì™¸: {e}")
        try:
            threads_browser.terminate()
        except Exception:
            pass
        return False

    threads_browser.terminate()
    return True


def main():
    try:
        news_info = crawl_newspic_ai_contents()
        print(f"í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ê°œìˆ˜: {len(news_info)}")
        upload_news_into_threads(news_info)
    except Exception as e:
        logging.warning(f"[main] 1ì°¨ ì‹œë„ ì‹¤íŒ¨ â†’ ì¬ì‹œë„: {e}")
        time.sleep(3)
        news_info = crawl_newspic_ai_contents()
        upload_news_into_threads(news_info)


if __name__ == "__main__":
    main()
    # scheduler = BlockingScheduler(timezone=SCHED_TZ)
    # # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€, ë¯¸ìŠ¤íŒŒì´ì–´ í—ˆìš©
    # scheduler.add_job(
    #     main,
    #     CronTrigger(minute=25, second=0),
    #     max_instances=1,
    #     coalesce=True,
    #     misfire_grace_time=120,
    # )
    # logging.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì‹œ 25ë¶„ 00ì´ˆ)")
    # scheduler.start()
